# Legal RAG Chatbot âš–ï¸

A production-ready Retrieval-Augmented Generation (RAG) chatbot for querying company policy documents. Built with **LangChain**, **OpenAI**, **ChromaDB**, and **Streamlit**.

## âœ¨ Features

- ğŸ“„ **PDF Processing**: Extracts text, sections, and metadata from policy documents
- ğŸ§  **Semantic Search**: Uses HuggingFace embeddings for intelligent document retrieval
- ğŸ’¾ **Persistent Storage**: Local ChromaDB vector store
- ğŸ¯ **Confidence Scoring**: Evaluates answer quality with multiple metrics
- ğŸ“š **Citation Tracking**: Provides section numbers and page references
- ğŸš¦ **Guardrails**: Prevents hallucinations and ensures grounded answers
- ğŸ–¥ï¸ **Interactive UI**: Clean Streamlit interface for testing

## ğŸ—ï¸ Architecture

```
User Question
     â†“
[Retriever] â†’ Search ChromaDB
     â†“
[Top-K Documents Retrieved]
     â†“
[LLM Client] â†’ Generate Answer with Context
     â†“
[Confidence Scorer] â†’ Evaluate Quality
     â†“
[Structured Answer] â†’ Citations + Confidence
```

## ğŸš€ Quick Start

### 1. Prerequisites

- Python 3.8+
- OpenAI API key

### 2. Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd legal-rag-chatbot

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configuration

```bash
# Copy environment template
cp .env.example .env

# Edit .env and add your OpenAI API key
OPENAI_API_KEY=sk-your-key-here
```

### 4. Add Your Documents

Place your PDF policy documents in `data/raw_docs/`:

```bash
data/raw_docs/
â”œâ”€â”€ policy_1.pdf
â”œâ”€â”€ policy_2.pdf
â””â”€â”€ policy_3.pdf
```

### 5. Run the Application

```bash
# Start Streamlit UI
streamlit run src/app.py
```

Or use the command-line interface:

```python
from src.ingestion.ingest_pipeline import ingest_all_documents
from src.rag.qa_pipeline import ask_question

# Ingest documents
ingest_all_documents()

# Ask a question
answer = ask_question("What is the data retention policy?")
print(answer.to_display())
```

## ğŸ“– Usage Guide

### Ingesting Documents

1. Place PDFs in `data/raw_docs/`
2. Click "Ingest Documents" in the sidebar
3. Wait for processing to complete

### Asking Questions

Example questions:
- *"What is the data retention policy?"*
- *"What are the requirements for data encryption?"*
- *"Who must I notify in case of a data breach?"*
- *"What is Section 7.2 about?"*

### Understanding Confidence Scores

- **High (â‰¥80%)**: Answer is well-supported with strong citations
- **Medium (60-79%)**: Answer is reasonable but may lack some context
- **Low (<60%)**: Answer may be incomplete or uncertain

## ğŸ”§ Advanced Configuration

### Environment Variables

Edit `.env` to customize:

```bash
# Model Settings
OPENAI_MODEL=gpt-4              # or gpt-3.5-turbo
EMBEDDING_MODEL=all-MiniLM-L6-v2

# RAG Settings
CHUNK_SIZE=1000                 # Characters per chunk
CHUNK_OVERLAP=200              # Overlap between chunks
TOP_K_RESULTS=5                # Documents to retrieve
CONFIDENCE_THRESHOLD=0.7       # Minimum confidence to answer

# Logging
LOG_LEVEL=INFO                 # DEBUG, INFO, WARNING, ERROR
```

### Programmatic Usage

```python
from src.rag.qa_pipeline import QAPipeline
from src.vectorstore.chroma_manager import ChromaManager

# Initialize pipeline
pipeline = QAPipeline(use_llm_eval=True)  # Enable LLM-based evaluation

# Ask question
answer = pipeline.answer_question(
    question="What is the data classification policy?",
    top_k=5,
    min_confidence=0.7
)

# Access answer components
print(f"Answer: {answer.answer}")
print(f"Confidence: {answer.confidence:.2%}")
print(f"Citations: {[c.format() for c in answer.citations]}")

# Filter by section
answer = pipeline.answer_with_filter(
    question="What are the access controls?",
    section_number="7.2"
)
```

### Managing Documents

```python
from src.vectorstore.chroma_manager import ChromaManager

chroma = ChromaManager()

# List all documents
docs = chroma.list_documents()
print(f"Documents: {docs}")

# Get statistics
stats = chroma.get_collection_stats()
print(stats)

# Delete a document
chroma.delete_document("policy_1")

# Reset database (WARNING: deletes all data)
chroma.reset_collection()
```

## ğŸ“ Project Structure

```
legal-rag-chatbot/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_docs/              # Place PDF files here
â”‚   â”œâ”€â”€ chroma_db/             # Vector database (auto-created)
â”‚   â””â”€â”€ processed/             # Metadata backups
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/             # PDF processing & chunking
â”‚   â”œâ”€â”€ embeddings/            # HuggingFace embeddings
â”‚   â”œâ”€â”€ vectorstore/           # ChromaDB manager
â”‚   â”œâ”€â”€ rag/                   # RAG pipeline components
â”‚   â”œâ”€â”€ models/                # Pydantic schemas
â”‚   â”œâ”€â”€ utils/                 # Configuration & logging
â”‚   â””â”€â”€ app.py                 # Streamlit UI
â”‚
â”œâ”€â”€ logs/                      # Application logs
â”œâ”€â”€ .env                       # Configuration (create from .env.example)
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸ§ª Testing

### CLI Testing

```bash
# Ingest documents
python -m src.ingestion.ingest_pipeline

# Test a question
python -c "from src.rag.qa_pipeline import ask_question; print(ask_question('What is Section 1 about?').to_display())"
```

### Interactive Testing

```bash
# Launch Streamlit UI
streamlit run src/app.py
```

## ğŸ› ï¸ Troubleshooting

### Issue: "No PDF files found"

**Solution**: Ensure PDFs are in `data/raw_docs/` with `.pdf` extension

### Issue: "OpenAI API key not found"

**Solution**: Check `.env` file has `OPENAI_API_KEY=sk-...`

### Issue: "ImportError: No module named ..."

**Solution**: Reinstall dependencies:
```bash
pip install --upgrade -r requirements.txt
```

### Issue: Low confidence answers

**Solutions**:
1. Increase `TOP_K_RESULTS` in `.env`
2. Reduce `CHUNK_SIZE` for better granularity
3. Enable LLM evaluation: `use_llm_eval=True`

## ğŸ” Security Considerations

- **API Keys**: Never commit `.env` to version control
- **Data Privacy**: All processing is local except OpenAI API calls
- **Access Control**: Implement authentication for production use
- **Audit Logging**: All queries are logged to `logs/`

## ğŸš€ Production Deployment

### Option 1: Streamlit Cloud

1. Push code to GitHub
2. Connect to Streamlit Cloud
3. Add OpenAI API key in secrets
4. Deploy

### Option 2: Docker

```dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["streamlit", "run", "src/app.py", "--server.port=8501"]
```

### Option 3: FastAPI Backend

Convert to REST API:

```python
from fastapi import FastAPI
from src.rag.qa_pipeline import QAPipeline

app = FastAPI()
pipeline = QAPipeline()

@app.post("/ask")
async def ask_question(question: str):
    answer = pipeline.answer_question(question)
    return answer.dict()
```

## ğŸ“Š Performance Optimization

- **Embedding Cache**: First run downloads model (~100MB)
- **ChromaDB**: Indexed for fast retrieval
- **Batch Processing**: Use `ingest_directory()` for multiple files
- **GPU Support**: Enable for faster embeddings (optional)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- **LangChain**: RAG framework
- **OpenAI**: LLM API
- **ChromaDB**: Vector database
- **Sentence Transformers**: Embeddings
- **Streamlit**: UI framework

## ğŸ“§ Support

For issues or questions:
- Create an issue on GitHub
- Check logs in `logs/` directory
- Review configuration in `.env`

---

**Built with â¤ï¸ for better policy management**